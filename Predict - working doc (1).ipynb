{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ejR4GG-6yFT"
      },
      "source": [
        "**Library Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy9JR4qjaD0c"
      },
      "source": [
        "# Libraries for data loading, data manipulation and data visulisation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Libraries for data preparation and model building\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import *\n",
        "from sklearn.linear_model import *\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew #for some statistics\n",
        "import seaborn as sns\n",
        "# Setting global constants to ensure notebook results are reproducible\n",
        "PARAMETER_CONSTANT = 42"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2erltz7aaKaP"
      },
      "source": [
        "import nltk\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "# set plot style\n",
        "sns.set()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wansti_o63tD"
      },
      "source": [
        "**Importing Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGhG8mKQtK7W"
      },
      "source": [
        "import io\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test_with_no_labels.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwQs6KUu7DV4"
      },
      "source": [
        "**Data Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BiE4_GLXtL1n",
        "outputId": "f5331c99-6506-43a3-bc22-7210dabf33ec"
      },
      "source": [
        "# look at data statistics\n",
        "df_train.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                            message  tweetid\n",
              "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
              "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
              "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
              "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
              "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHBm2VDUvqdS",
        "outputId": "ba8f69bf-fff2-4531-bf65-006bc6ade60e"
      },
      "source": [
        "# look at All columns in tthe train data\n",
        "df_train.columns"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentiment', 'message', 'tweetid'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "uZ19aKzzvvqg",
        "outputId": "a75f6531-ba89-40b3-d24d-624fdc754e0e"
      },
      "source": [
        "# look at the sum of data observations\n",
        "df_train.describe()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>15819.000000</td>\n",
              "      <td>15819.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.917504</td>\n",
              "      <td>501719.433656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.836537</td>\n",
              "      <td>289045.983132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>253207.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>502291.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>753769.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>999888.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          sentiment        tweetid\n",
              "count  15819.000000   15819.000000\n",
              "mean       0.917504  501719.433656\n",
              "std        0.836537  289045.983132\n",
              "min       -1.000000       6.000000\n",
              "25%        1.000000  253207.500000\n",
              "50%        1.000000  502291.000000\n",
              "75%        1.000000  753769.000000\n",
              "max        2.000000  999888.000000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYoHqr1av0X5",
        "outputId": "902c646e-a722-420a-d44d-a0a56f333388"
      },
      "source": [
        "# check for null values in the columns\n",
        "df_train.isnull().sum()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment    0\n",
              "message      0\n",
              "tweetid      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjgZLnDSv6BB",
        "outputId": "6460f7be-006f-4c44-9fa5-73ed6d6df33e"
      },
      "source": [
        "# Merge train and test data to easily clean and split data \n",
        "df = pd.concat([df_train, df_test])\n",
        "df.head"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of        sentiment                                            message  tweetid\n",
              "0            1.0  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
              "1            1.0  It's not like we lack evidence of anthropogeni...   126103\n",
              "2            2.0  RT @RawStory: Researchers say we have three ye...   698562\n",
              "3            1.0  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
              "4            1.0  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954\n",
              "...          ...                                                ...      ...\n",
              "10541        NaN  RT @BrittanyBohrer: Brb, writing a poem about ...   895714\n",
              "10542        NaN  2016: the year climate change came home: Durin...   875167\n",
              "10543        NaN  RT @loop_vanuatu: Pacific countries positive a...    78329\n",
              "10544        NaN  RT @xanria_00018: You’re so hot, you must be t...   867455\n",
              "10545        NaN  RT @chloebalaoing: climate change is a global ...   470892\n",
              "\n",
              "[26365 rows x 3 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbGudi4CwAmc",
        "outputId": "38b6c289-a54b-4511-ce4c-45479f0b7602"
      },
      "source": [
        "# Checking how Nulls in each column changed with the training and testing data merged\n",
        "df.isnull().sum()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment    10546\n",
              "message          0\n",
              "tweetid          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfP1nBCZ6rm8"
      },
      "source": [
        "**NLTK Download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3QX4PA6zCQj",
        "outputId": "74657bde-f257-477a-dc48-ab176879c61e"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpyOyJJozSsG"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "mRDr9Ipx0TgT",
        "outputId": "37cd4016-70dd-4195-80e5-e358c09a64cf"
      },
      "source": [
        "#Sentiment Occurence Count\n",
        "df_train['sentiment'].value_counts().plot(kind = 'bar')\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEG7RWfY6j-D"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNPb6k9d4f5K"
      },
      "source": [
        "#URL Removal\n",
        "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
        "subs_url = r'url-web'\n",
        "df['message'] = df['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCZCcOM4syt"
      },
      "source": [
        "#Lower\n",
        "df['message'] = df['message'].str.lower()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mpM4ZJQ5BMS",
        "outputId": "ebed731c-4f19-4023-ec16-1d3023900b18"
      },
      "source": [
        "#Remove Punctuation\n",
        "import string\n",
        "print(string.punctuation)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysMQKntJ5OFS"
      },
      "source": [
        "def remove_punctuation(post):\n",
        "    return ''.join([l for l in post if l not in string.punctuation])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qkyDXOQJ5RpB",
        "outputId": "7c9d045c-aef5-418a-814d-43425d0de86d"
      },
      "source": [
        "df['message'] = df['message'].apply(remove_punctuation)\n",
        "df['message'].iloc[4]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'rt soynoviodetodas its 2016 and a racist sexist climate change denying bigot is leading in the polls electionnight'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx3_jHmP6UwK"
      },
      "source": [
        "**TokenIzation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2k_8CGi6bNU"
      },
      "source": [
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k6b8Yxf7RUR"
      },
      "source": [
        "tokeniser = TreebankWordTokenizer()\n",
        "df['tokens'] = df['message'].apply(tokeniser.tokenize)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZrkSRPS7tnH",
        "outputId": "09c2a201-f2a0-4fd2-f532-050a59550526"
      },
      "source": [
        "df['tokens'].iloc[4]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'soynoviodetodas',\n",
              " 'its',\n",
              " '2016',\n",
              " 'and',\n",
              " 'a',\n",
              " 'racist',\n",
              " 'sexist',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'denying',\n",
              " 'bigot',\n",
              " 'is',\n",
              " 'leading',\n",
              " 'in',\n",
              " 'the',\n",
              " 'polls',\n",
              " 'electionnight']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qhCUW59S8GhV",
        "outputId": "98340f50-0f0b-468a-c066-0d0dfec3f0e9"
      },
      "source": [
        "#View of token column\n",
        "df.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>rt rawstory researchers say we have three year...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[rt, rawstory, researchers, say, we, have, thr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>todayinmaker wired  2016 was a pivotal year in...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[todayinmaker, wired, 2016, was, a, pivotal, y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>rt soynoviodetodas its 2016 and a racist sexis...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[rt, soynoviodetodas, its, 2016, and, a, racis...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                             tokens\n",
              "0        1.0  ...  [polyscimajor, epa, chief, doesnt, think, carb...\n",
              "1        1.0  ...  [its, not, like, we, lack, evidence, of, anthr...\n",
              "2        2.0  ...  [rt, rawstory, researchers, say, we, have, thr...\n",
              "3        1.0  ...  [todayinmaker, wired, 2016, was, a, pivotal, y...\n",
              "4        1.0  ...  [rt, soynoviodetodas, its, 2016, and, a, racis...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyx0S0918Y6Z"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rhGJAY68btG"
      },
      "source": [
        "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3uZRD0n9168"
      },
      "source": [
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxqPC8nB9PeY"
      },
      "source": [
        "def df_stemmer(words, stemmer):\n",
        "    return [stemmer.stem(word) for word in words]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4ET7dDK9ZLj"
      },
      "source": [
        "df['stem'] = df['tokens'].apply(df_stemmer, args=(stemmer, ))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcyXbiq-dh7"
      },
      "source": [
        "Print off the results of the stemmer to see what we have done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lypsNfjJ-BaE",
        "outputId": "ed97839e-020e-4b90-c2f8-721bd2c68a17"
      },
      "source": [
        "for i, t in enumerate(df.iloc[2]['tokens']):    \n",
        "    print ('{:20s} --> {:10s}'.format(t, df.iloc[2]['stem'][i]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rt                   --> rt        \n",
            "rawstory             --> rawstori  \n",
            "researchers          --> research  \n",
            "say                  --> say       \n",
            "we                   --> we        \n",
            "have                 --> have      \n",
            "three                --> three     \n",
            "years                --> year      \n",
            "to                   --> to        \n",
            "act                  --> act       \n",
            "on                   --> on        \n",
            "climate              --> climat    \n",
            "change               --> chang     \n",
            "before               --> befor     \n",
            "it                   --> it        \n",
            "’                    --> ’         \n",
            "s                    --> s         \n",
            "too                  --> too       \n",
            "late                 --> late      \n",
            "urlweb               --> urlweb    \n",
            "urlweb…              --> urlweb…   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mxvuW-Kh-ptY",
        "outputId": "d7ebd542-751b-4ae0-b2bd-826459c0a4dc"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tokens</th>\n",
              "      <th>stem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
              "      <td>[it, not, like, we, lack, evid, of, anthropoge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>rt rawstory researchers say we have three year...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[rt, rawstory, researchers, say, we, have, thr...</td>\n",
              "      <td>[rt, rawstori, research, say, we, have, three,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>todayinmaker wired  2016 was a pivotal year in...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[todayinmaker, wired, 2016, was, a, pivotal, y...</td>\n",
              "      <td>[todayinmak, wire, 2016, was, a, pivot, year, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>rt soynoviodetodas its 2016 and a racist sexis...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[rt, soynoviodetodas, its, 2016, and, a, racis...</td>\n",
              "      <td>[rt, soynoviodetoda, it, 2016, and, a, racist,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                               stem\n",
              "0        1.0  ...  [polyscimajor, epa, chief, doesnt, think, carb...\n",
              "1        1.0  ...  [it, not, like, we, lack, evid, of, anthropoge...\n",
              "2        2.0  ...  [rt, rawstori, research, say, we, have, three,...\n",
              "3        1.0  ...  [todayinmak, wire, 2016, was, a, pivot, year, ...\n",
              "4        1.0  ...  [rt, soynoviodetoda, it, 2016, and, a, racist,...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LYEOzEH-424"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioF2nR4E-_ue"
      },
      "source": [
        "A very similar operation to stemming is called lemmatization. Lemmatizing is the process of grouping words of similar meaning together. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GMlKY1O_A_y",
        "outputId": "9d45670a-ebf3-42ec-95aa-9fa425402fe8"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ee2vZe_KGV"
      },
      "source": [
        "Let's lemmatize all of the words in the DF dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl-NZzdv_Y7p"
      },
      "source": [
        "def df_lemma(words, lemmatizer):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]    "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX-A2gq8_wO4"
      },
      "source": [
        "df['lemma'] = df['tokens'].apply(df_lemma, args=(lemmatizer, ))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-X6v94R_fUq"
      },
      "source": [
        "Now we will print out the results of the lemmatization to see what we have done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTKQUjxL_ih0",
        "outputId": "5983d9d9-a828-44ab-e0f7-9661241777bc"
      },
      "source": [
        "for i, t in enumerate(df.iloc[2]['tokens']):    \n",
        "    print ('{:20s} --> {:10s}'.format(t, df.iloc[2]['lemma'][i]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rt                   --> rt        \n",
            "rawstory             --> rawstory  \n",
            "researchers          --> researcher\n",
            "say                  --> say       \n",
            "we                   --> we        \n",
            "have                 --> have      \n",
            "three                --> three     \n",
            "years                --> year      \n",
            "to                   --> to        \n",
            "act                  --> act       \n",
            "on                   --> on        \n",
            "climate              --> climate   \n",
            "change               --> change    \n",
            "before               --> before    \n",
            "it                   --> it        \n",
            "’                    --> ’         \n",
            "s                    --> s         \n",
            "too                  --> too       \n",
            "late                 --> late      \n",
            "urlweb               --> urlweb    \n",
            "urlweb…              --> urlweb…   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "JkAsG7Kd_6cP",
        "outputId": "accb6c6a-0535-48c8-a8cf-2124108f2586"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tokens</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
              "      <td>[it, not, like, we, lack, evid, of, anthropoge...</td>\n",
              "      <td>[it, not, like, we, lack, evidence, of, anthro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>rt rawstory researchers say we have three year...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[rt, rawstory, researchers, say, we, have, thr...</td>\n",
              "      <td>[rt, rawstori, research, say, we, have, three,...</td>\n",
              "      <td>[rt, rawstory, researcher, say, we, have, thre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>todayinmaker wired  2016 was a pivotal year in...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[todayinmaker, wired, 2016, was, a, pivotal, y...</td>\n",
              "      <td>[todayinmak, wire, 2016, was, a, pivot, year, ...</td>\n",
              "      <td>[todayinmaker, wired, 2016, wa, a, pivotal, ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>rt soynoviodetodas its 2016 and a racist sexis...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[rt, soynoviodetodas, its, 2016, and, a, racis...</td>\n",
              "      <td>[rt, soynoviodetoda, it, 2016, and, a, racist,...</td>\n",
              "      <td>[rt, soynoviodetodas, it, 2016, and, a, racist...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                              lemma\n",
              "0        1.0  ...  [polyscimajor, epa, chief, doesnt, think, carb...\n",
              "1        1.0  ...  [it, not, like, we, lack, evidence, of, anthro...\n",
              "2        2.0  ...  [rt, rawstory, researcher, say, we, have, thre...\n",
              "3        1.0  ...  [todayinmaker, wired, 2016, wa, a, pivotal, ye...\n",
              "4        1.0  ...  [rt, soynoviodetodas, it, 2016, and, a, racist...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21rRtaEMC8xV"
      },
      "source": [
        "**Stop Words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxrxrrzEDBKJ"
      },
      "source": [
        "Stop words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return a vast amount of unnecessary information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mWHi_FaDK1c"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtVUrS35DN0f"
      },
      "source": [
        "def remove_stop_words(tokens):    \n",
        "    return [t for t in tokens if t not in stopwords.words('english')]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zit9K0_ECZZ",
        "outputId": "9c56ac77-8044-427f-e031-457531a3a161"
      },
      "source": [
        "nltk.download(['punkt','stopwords'])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbnY_TbmDWEY"
      },
      "source": [
        "df['stem'] = df['tokens'].apply(remove_stop_words)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "zxw8FYpYEXsp",
        "outputId": "48010b3e-3062-4303-c852-32f4f1b262c1"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tokens</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
              "      <td>[like, lack, evidence, anthropogenic, global, ...</td>\n",
              "      <td>[it, not, like, we, lack, evidence, of, anthro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>rt rawstory researchers say we have three year...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[rt, rawstory, researchers, say, we, have, thr...</td>\n",
              "      <td>[rt, rawstory, researchers, say, three, years,...</td>\n",
              "      <td>[rt, rawstory, researcher, say, we, have, thre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>todayinmaker wired  2016 was a pivotal year in...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[todayinmaker, wired, 2016, was, a, pivotal, y...</td>\n",
              "      <td>[todayinmaker, wired, 2016, pivotal, year, war...</td>\n",
              "      <td>[todayinmaker, wired, 2016, wa, a, pivotal, ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>rt soynoviodetodas its 2016 and a racist sexis...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[rt, soynoviodetodas, its, 2016, and, a, racis...</td>\n",
              "      <td>[rt, soynoviodetodas, 2016, racist, sexist, cl...</td>\n",
              "      <td>[rt, soynoviodetodas, it, 2016, and, a, racist...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                              lemma\n",
              "0        1.0  ...  [polyscimajor, epa, chief, doesnt, think, carb...\n",
              "1        1.0  ...  [it, not, like, we, lack, evidence, of, anthro...\n",
              "2        2.0  ...  [rt, rawstory, researcher, say, we, have, thre...\n",
              "3        1.0  ...  [todayinmaker, wired, 2016, wa, a, pivotal, ye...\n",
              "4        1.0  ...  [rt, soynoviodetodas, it, 2016, and, a, racist...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHyZMXs0FDhj"
      },
      "source": [
        "**Text Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-wU2gZdFITk"
      },
      "source": [
        "*Bag of Words*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOQAvHQfFgvZ"
      },
      "source": [
        "def bag_of_words_count(words, word_dict={}):\n",
        "    \"\"\" this function takes in a list of words and returns a dictionary \n",
        "        with each word as a key, and the value represents the number of \n",
        "        times that word appeared\"\"\"\n",
        "    for word in words:\n",
        "        if word in word_dict.keys():\n",
        "            word_dict[word] += 1\n",
        "        else:\n",
        "            word_dict[word] = 1\n",
        "    return word_dict"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk_25sw8I_Di",
        "outputId": "bb7ed42a-3f0a-424e-e902-52126c2f6408"
      },
      "source": [
        "type_labels = list(df_train.sentiment.unique())\n",
        "print(type_labels)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 0, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "aMN6PJYAFpfR",
        "outputId": "4e3e9f35-4ef9-48f0-d4cc-69513ec9915f"
      },
      "source": [
        "outcome = {}\n",
        "for pp in type_labels:\n",
        "    df_train = df_train.groupby('sentiment')\n",
        "    outcome[pp] = {}\n",
        "    for row in df_train.get_group(pp)['message']:\n",
        "        outcome[pp] = bag_of_words_count(row, outcome[pp])  "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-d4866d1a19a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtype_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutcome\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         raise AttributeError(\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{attr}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameGroupBy' object has no attribute 'groupby'"
          ]
        }
      ]
    }
  ]
}